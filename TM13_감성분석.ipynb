{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyP8IrlBWSudtpyoLxLIItNR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#영화 리뷰 데이터(NSMC)를 활용한 감성 분석\n","---"],"metadata":{"id":"048iZlBHtcdS"}},{"cell_type":"markdown","source":["#(1) Step1. 데이터 로드와 구조 확인"],"metadata":{"id":"cdU8CIHGZ2wo"}},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"id":"9ItpIkOsYjGa"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5_ACcj2jT62j"},"outputs":[],"source":["import pandas as pd\n","\n","# 데이터셋 다운로드 경로\n","train_url = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\"\n","test_url = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\"\n","\n","# 데이터 로드\n","train_data = pd.read_csv(train_url, sep=\"\\t\", encoding=\"utf-8\")\n","test_data = pd.read_csv(test_url, sep=\"\\t\", encoding=\"utf-8\")\n","\n","# 데이터 확인\n","print(train_data.head())\n","print(test_data.head())\n"]},{"cell_type":"markdown","source":["#(2) Step2: 데이터 전처리 (결측치 제거)"],"metadata":{"id":"rT2Y7UuQaKlk"}},{"cell_type":"code","source":["# 불필요한 결측치 제거\n","train_data = train_data.dropna()\n","test_data = test_data.dropna()"],"metadata":{"id":"XabnvMW3Z-4I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#(3) Step3: 텍스트 데이터 전처리 (정규식, 형태소분석 및 어간추출)"],"metadata":{"id":"ol361gQzaUMN"}},{"cell_type":"code","source":["import re\n","from konlpy.tag import Okt\n","okt = Okt()\n","\n","stopwords = set([\n","    '영화', '보다', '있다', '이다', '하다','그냥', '정말', '좀', '역시', '이건',\n","    '너무', '이', '그', '가', '를', '은', '는', '의', '에', '도', '으로', '하고',\n","    '그리고', '하지만', '더', '또', '에서', '보다도'\n","])\n","\n","# 텍스트 전처리 함수\n","def preprocess_text(text):\n","    text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", text)  # 한글 및 공백 제외 제거\n","    tokens = okt.morphs(text, stem=True)  # 형태소 분석 및 어간 추출\n","    tokens = [word for word in tokens if word not in stopwords]  # 불용어 제거\n","    return \" \".join(tokens)\n","# 전처리 적용\n","train_data[\"cleaned_document\"] = train_data[\"document\"].apply(preprocess_text)\n","test_data[\"cleaned_document\"] = test_data[\"document\"].apply(preprocess_text)\n","# 전처리 결과 확인\n","print(train_data[[\"document\", \"cleaned_document\"]].head())\n"],"metadata":{"id":"cqaJsF1qUhSG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#(4) Step4: 텍스트 벡터화"],"metadata":{"id":"1nwerLWqc4oT"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","# TF-IDF 벡터화\n","vectorizer = TfidfVectorizer(max_features=5000)\n","X_train = vectorizer.fit_transform(train_data[\"cleaned_document\"])\n","X_test = vectorizer.transform(test_data[\"cleaned_document\"])\n","# 라벨\n","y_train = train_data[\"label\"]\n","y_test = test_data[\"label\"]\n","\n","print(f\"훈련 데이터 TF-IDF 크기: {X_train.shape}\")\n","print(f\"테스트 데이터 TF-IDF 크기: {X_test.shape}\")\n"],"metadata":{"id":"dtqKuwbJcpGx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#(5) Step5: 머신러닝 모델 학습"],"metadata":{"id":"2VFhdL5HdDM8"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","# 로지스틱 회귀 모델 학습\n","model = LogisticRegression(max_iter=1000)\n","model.fit(X_train, y_train)\n","# 예측 및 성능 평가\n","y_pred = model.predict(X_test)\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"],"metadata":{"id":"qMy0ZrUycwq5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#(6) Step6: 시각화"],"metadata":{"id":"sahoSQnvdLXR"}},{"cell_type":"code","source":["!apt-get update && apt-get install -y fonts-nanum"],"metadata":{"id":"1IXIR_ThdBf2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n","# 긍정/부정 리뷰 분리\n","positive_reviews = \" \".join(train_data[train_data[\"label\"] == 1][\"cleaned_document\"])\n","negative_reviews = \" \".join(train_data[train_data[\"label\"] == 0][\"cleaned_document\"])\n","\n","# WordCloud 생성\n","wordcloud_positive = WordCloud(font_path=font_path, background_color='white').generate(positive_reviews)\n","wordcloud_negative = WordCloud(font_path=font_path, background_color='black').generate(negative_reviews)\n","\n","# 시각화\n","plt.figure(figsize=(10, 5))\n","plt.subplot(1, 2, 1)\n","plt.title(\"Positive Reviews\")\n","plt.imshow(wordcloud_positive, interpolation=\"bilinear\")\n","plt.axis(\"off\")\n","\n","plt.subplot(1, 2, 2)\n","plt.title(\"Negative Reviews\")\n","plt.imshow(wordcloud_negative, interpolation=\"bilinear\")\n","plt.axis(\"off\")\n","plt.show()"],"metadata":{"id":"DRHqkUYAcy5f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10, 5))\n","plt.subplot(1, 2, 1)\n","plt.title(\"Positive Reviews\")\n","plt.imshow(wordcloud_positive, interpolation=\"bilinear\")\n","plt.axis(\"off\")\n","plt.subplot(1, 2, 2)\n","plt.title(\"Negative Reviews\")\n","plt.imshow(wordcloud_negative, interpolation=\"bilinear\")\n","plt.axis(\"off\")\n","plt.show()"],"metadata":{"id":"Mgy6T7FKdWQ4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","#전체코드\n"],"metadata":{"id":"PZdrwKSpgyH3"}},{"cell_type":"code","source":["import pandas as pd\n","# 데이터셋 다운로드 경로\n","train_url = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\"\n","test_url = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\"\n","# 데이터 로드\n","train_data = pd.read_csv(train_url, sep=\"\\t\")\n","test_data = pd.read_csv(test_url, sep=\"\\t\")\n","# 데이터 확인\n","print(train_data.head())\n","print(test_data.head())\n","\n","train_data = train_data.dropna()\n","test_data = test_data.dropna()\n","\n","import re\n","from konlpy.tag import Okt\n","okt = Okt()\n","# 텍스트 전처리 함수\n","def preprocess_text(text):\n","    text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", text)  # 한글 및 공백 제외 제거\n","    tokens = okt.morphs(text, stem=True)  # 형태소 분석 및 어간 추출\n","    return \" \".join(tokens)\n","# 전처리 적용\n","train_data[\"cleaned_document\"] = train_data[\"document\"].apply(preprocess_text)\n","test_data[\"cleaned_document\"] = test_data[\"document\"].apply(preprocess_text)\n","print(train_data[[\"document\", \"cleaned_document\"]].head()) #결과 출력\n","\n","import re\n","from konlpy.tag import Okt\n","okt = Okt()\n","\n","stopwords = set([\n","    '영화', '보다', '있다', '이다', '그냥', '정말', '좀', '역시', '이건',\n","    '너무', '이', '그', '가', '를', '은', '는', '의', '에', '도', '으로', '하고',\n","    '그리고', '하지만', '더', '또', '에서', '보다도'\n","])\n","\n","# 텍스트 전처리 함수\n","def preprocess_text(text):\n","    text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", text)  # 한글 및 공백 제외 제거\n","    tokens = okt.morphs(text, stem=True)  # 형태소 분석 및 어간 추출\n","    tokens = [word for word in tokens if word not in stopwords]  # 불용어 제거\n","    return \" \".join(tokens)\n","# 전처리 적용\n","train_data[\"cleaned_document\"] = train_data[\"document\"].apply(preprocess_text)\n","test_data[\"cleaned_document\"] = test_data[\"document\"].apply(preprocess_text)\n","# 전처리 결과 확인\n","print(train_data[[\"document\", \"cleaned_document\"]].head())\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","# TF-IDF 벡터화\n","vectorizer = TfidfVectorizer(max_features=5000)\n","X_train = vectorizer.fit_transform(train_data[\"cleaned_document\"])\n","X_test = vectorizer.transform(test_data[\"cleaned_document\"])\n","# 라벨\n","y_train = train_data[\"label\"]\n","y_test = test_data[\"label\"]\n","\n","print(f\"훈련 데이터 TF-IDF 크기: {X_train.shape}\")\n","print(f\"테스트 데이터 TF-IDF 크기: {X_test.shape}\")\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","# 로지스틱 회귀 모델 학습\n","model = LogisticRegression(max_iter=1000)\n","model.fit(X_train, y_train)\n","# 예측 및 성능 평가\n","y_pred = model.predict(X_test)\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"],"metadata":{"id":"-XJLICFWg1su"},"execution_count":null,"outputs":[]}]}