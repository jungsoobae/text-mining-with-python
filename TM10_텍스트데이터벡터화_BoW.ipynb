{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"mount_file_id":"1nQUsE-K03Rd0RCSCOuJDWkYlawffAwJK","authorship_tag":"ABX9TyM4T1fhUV2wO+Htw7vZ8ggJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#10.2.2. Bag of Words êµ¬í˜„ ë‹¨ê³„"],"metadata":{"id":"8zf2mcnyqz0h"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HC0u39O_amTa"},"outputs":[],"source":["pip install scikit-learn pandas numpy"]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer()  # BoW ìƒì„±ê¸°\n","X = vectorizer.fit_transform([\"ë‚˜ëŠ” ì‚¬ê³¼ë¥¼ ì¢‹ì•„í•œë‹¤.\", \"ë‚˜ëŠ” ë°”ë‚˜ë‚˜ë¥¼ ì¢‹ì•„í•œë‹¤.\"])  # ë¬¸ì¥ ì…ë ¥\n","print(X.toarray())  # BoW í–‰ë ¬ ì¶œë ¥\n"],"metadata":{"id":"Ia7sGo-0csLG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#ì˜ˆì œ1)\tBoW êµ¬í˜„ ë‹¨ê³„"],"metadata":{"id":"FR6rIbdRgaEB"}},{"cell_type":"code","source":["# 1ë‹¨ê³„ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n","from sklearn.feature_extraction.text import CountVectorizer\n","import pandas as pd\n","\n","# ğŸ“Œ 2ë‹¨ê³„: ë¬¸ì¥ ë°ì´í„° ì¤€ë¹„\n","sentences = [\n","    \"ë‚˜ëŠ” ì‚¬ê³¼ë¥¼ ì¢‹ì•„í•œë‹¤.\",\n","    \"ë‚˜ëŠ” ë°”ë‚˜ë‚˜ë¥¼ ì¢‹ì•„í•œë‹¤.\",\n","    \"ë‚˜ëŠ” ë°”ë‚˜ë‚˜ì™€ ì‚¬ê³¼ë¥¼ ì¢‹ì•„í•œë‹¤.\"\n","]\n","\n","# ğŸ“Œ 3ë‹¨ê³„: CountVectorizerë¥¼ ì‚¬ìš©í•˜ì—¬ BoW ìƒì„±\n","vectorizer = CountVectorizer()  # BoW ë³€í™˜ê¸° ìƒì„±\n","X = vectorizer.fit_transform(sentences)  # ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ë³€í™˜\n","print(X.toarray())  # BoW í–‰ë ¬ ì¶œë ¥\n","\n","# ğŸ“Œ 4ë‹¨ê³„: ë‹¨ì–´ ì‚¬ì „(Vocabulary) í™•ì¸\n","vocab = vectorizer.get_feature_names_out()\n","print(vocab)\n","\n","# ğŸ“Œ 5ë‹¨ê³„: BoW í–‰ë ¬ì„ ë³´ê¸° ì‰½ê²Œ DataFrameìœ¼ë¡œ ë³€í™˜\n","df_bow = pd.DataFrame(X.toarray(), columns=vocab)\n","\n","print(df_bow)"],"metadata":{"id":"VkwwyuuSgbgl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#10.3. Term Frequency-Inverse Document Frequency\n","      (TF-IDF)\n"],"metadata":{"id":"QnGFmxrerwX-"}},{"cell_type":"markdown","source":["##ì˜ˆì œ2)\tTF-IDF êµ¬í˜„ ë‹¨ê³„"],"metadata":{"id":"0gzo__ksglEN"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","\n","sentences = [\n","    \"ë‚˜ëŠ” ì‚¬ê³¼ë¥¼ ì¢‹ì•„í•œë‹¤.\",\n","    \"ë‚˜ëŠ” ë°”ë‚˜ë‚˜ë¥¼ ì¢‹ì•„í•œë‹¤.\",\n","    \"ë‚˜ëŠ” ë°”ë‚˜ë‚˜ì™€ ì‚¬ê³¼ë¥¼ ì¢‹ì•„í•œë‹¤.\"\n","]\n","#(3) TF-IDF ë³€í™˜ê¸°(TfidfVectorizer) ìƒì„± ë° ì ìš©\n","vectorizer = TfidfVectorizer()\n","x = vectorizer.fit_transform(sentences)\n","\n","vocab = vectorizer.get_feature_names_out()\n","\n","df_tfidf = pd.DataFrame(x.toarray(), columns=vocab)\n","print(df_tfidf)"],"metadata":{"id":"YRSsQzC0fhL1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","#10.4.1. ë„¤ì´ë²„ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ BoWì™€ TF-IDFë¡œ ë³€í™˜í•˜ê³  ë¹„êµí•˜ê¸°"],"metadata":{"id":"Q2LHjro7uhac"}},{"cell_type":"markdown","source":["##ì˜ˆì œ3)\të„¤ì´ë²„ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ BoWë¡œ ë¶„ì„í•˜ê¸°"],"metadata":{"id":"Ez32oF04vr9s"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","#CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n","file_path=\"/content/naver_news_headlines.csv\"\n","df = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n","\n","#ë‰´ìŠ¤ ì œëª© ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n","sentences = df[\"ë‰´ìŠ¤ ì œëª©\"].astype(str).tolist()\n","\n","#BoW(Bag of Words) ë³€í™˜ê¸° ìƒì„± ë° ì ìš©\n","vectorizer_bow = CountVectorizer()  # CountVectorizer ê°ì²´ ìƒì„± (BoW ë³€í™˜ê¸°)\n","X_bow = vectorizer_bow.fit_transform(sentences)  # ë‰´ìŠ¤ ì œëª© ë°ì´í„°ë¥¼ BoW í˜•ì‹\n","\n","#ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸(ì–´íœ˜ ì‚¬ì „) ìƒì„±\n","vocab_bow = vectorizer_bow.get_feature_names_out()\n","print(vocab_bow[:10])\n","#BoW í–‰ë ¬ì„ DataFrameìœ¼ë¡œ ë³€í™˜\n","df_bow = pd.DataFrame(X_bow.toarray(), columns=vocab_bow)\n","\n","#ê°€ì¥ ë§ì´ ë“±ì¥í•œ ë‹¨ì–´ 10ê°œ ì°¾ê¸° (BoW ê¸°ì¤€)\n","top10 = df_bow.sum().sort_values(ascending=False).head(10)\n","df_top10_words = pd.DataFrame(top10, columns=[\"ë¹ˆë„ìˆ˜\"]).reset_index()\n","df_top10_words.rename(columns={\"index\": \"ë‹¨ì–´\"}, inplace=True)\n","\n","#ê²°ê³¼ ì¶œë ¥\n","print(df_top10_words)  # ê°€ì¥ ë§ì´ ë“±ì¥í•œ ë‹¨ì–´ 10ê°œ ì¶œë ¥\n"],"metadata":{"id":"px4mUAqLuqHH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ì˜ˆì œ4)\të„¤ì´ë²„ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ TF-IDFë¡œ ë¶„ì„í•˜ê¸°"],"metadata":{"id":"zk03B8jmv8lx"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n","file_path=\"/content/naver_news_headlines.csv\"\n","df = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n","\n","#ë‰´ìŠ¤ ì œëª© ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n","sentences = df[\"ë‰´ìŠ¤ ì œëª©\"].astype(str).tolist()\n","#TF-IDF ë³€í™˜ê¸° ìƒì„± ë° ì ìš©\n","vectorizer_tf = TfidfVectorizer()\n","X_tf = vectorizer_tf.fit_transform(sentences)\n","#ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸(ì–´íœ˜ ì‚¬ì „) ìƒì„±\n","vocab_tf = vectorizer_tf.get_feature_names_out()\n","\n","#TF-IDF í–‰ë ¬ì„ DataFrameìœ¼ë¡œ ë³€í™˜\n","df_tf = pd.DataFrame(X_tf.toarray(), columns=vocab_tf)\n","\n","#ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ì–´ 10ê°œ ì°¾ê¸° (TF-IDF ê¸°ì¤€)\n","bow_top10 = df_tf.sum().sort_values(ascending=False).head(10)\n","df_bow_top10_words = pd.DataFrame(bow_top10, columns=[\"ì ìˆ˜\"]).reset_index()\n","df_bow_top10_words.rename(columns={\"index\": \"ë‹¨ì–´\"}, inplace=True)\n","#ìµœì¢… ê²°ê³¼ ì¶œë ¥\n","print(df_bow_top10_words) # ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ì–´ 10ê°œë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„ ì¶œë ¥\n"],"metadata":{"id":"xifLUsOVv_il"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","#10.4.2. BTS ê°€ì‚¬ë¥¼ ë°ì´í„° ë¶„ì„ìœ¼ë¡œ ì‚´í´ë³´ê¸°"],"metadata":{"id":"09vkh-I3wSje"}},{"cell_type":"markdown","source":["##ì˜ˆì œ5)\tBTS ê°€ì‚¬ BoW ë¶„ì„"],"metadata":{"id":"SUhMrtXywVo8"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","plt.rc('font', family='NanumBarunGothic')  # í•œê¸€ í°íŠ¸ ì„¤ì •\n","plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€\n"],"metadata":{"id":"GSCcVM-8wcvo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_path=\"/content/BTS01.csv\"\n","df = pd.read_csv(file_path, encoding=\"utf-8-sig\")  #CSVíŒŒì¼ì„ pandas\n","sentences = df[\"ê°€ì‚¬\"].astype(str).tolist()  # ê°€ì‚¬ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n"],"metadata":{"id":"c6_pKyHBwdzu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorizer_bow = CountVectorizer()  # BoW ë³€í™˜ê¸° ìƒì„±\n","X_bow = vectorizer_bow.fit_transform(sentences)  # ê°€ì‚¬ ë°ì´í„°ë¥¼ BoWë¡œ ë³€í™˜\n","vocab = vectorizer_bow.get_feature_names_out()  # ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸(ì–´íœ˜ ì‚¬ì „) ìƒì„±\n","df_bow = pd.DataFrame(X_bow.toarray(), columns=vocab)\n","print(df_bow)  # BoW ë°ì´í„° ì¶œë ¥\n"],"metadata":{"id":"MFW5QDJfwnvn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top_10_words = df_bow.sum().sort_values(ascending=False).head(10)\n","df_top_10_words = pd.DataFrame(top_10_words, columns=[\"ë¹ˆë„ìˆ˜\"]).reset_index()\n","df_top_10_words.rename(columns={\"index\": \"ë‹¨ì–´\"}, inplace=True)\n"],"metadata":{"id":"IhTligEMwtAv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","> í•œê¸€ í°íŠ¸ ì‚¬ìš©"],"metadata":{"id":"2OmHsE_Pw6UY"}},{"cell_type":"code","source":["!apt-get install -y fonts-nanum\n","!fc-cache -fv\n","!rm -rf ~/.cache/matplotlib"],"metadata":{"id":"ccjSjcjbw-PJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.rc('font', family='NanumBarunGothic')  # ë˜ëŠ” 'NanumGothic' ì‚¬ìš© ê°€ëŠ¥\n","plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ê°€ ê¹¨ì§€ì§€ ì•Šë„ë¡ ì„¤ì •\n"],"metadata":{"id":"9EogY8CVxAzV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","# ìƒìœ„ 10ê°œ ë‹¨ì–´ ë°” ì°¨íŠ¸ ì‹œê°í™”\n","plt.figure(figsize=(10, 6))\n","plt.bar(df_top_10_words[\"ë‹¨ì–´\"], df_top_10_words[\"ë¹ˆë„ìˆ˜\"], color=\"royalblue\")\n","plt.xlabel(\"ë‹¨ì–´\")\n","plt.ylabel(\"ë¹ˆë„ìˆ˜\")\n","plt.title(\"BTS ê°€ì‚¬ì—ì„œ ê°€ì¥ ë§ì´ ë“±ì¥í•œ ë‹¨ì–´ TOP 10\")\n","plt.show()\n"],"metadata":{"id":"DupXNt00wykT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","##ì˜ˆì œ5 ì „ì²´ ì½”ë“œ"],"metadata":{"id":"qzqqiD4-xmJa"}},{"cell_type":"code","source":["#(1) í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n","from sklearn.feature_extraction.text import CountVectorizer\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","plt.rc('font', family='NanumBarunGothic')  # í•œê¸€ í°íŠ¸ ì„¤ì •\n","plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€\n","\n","#ê°€ì‚¬ ë¶ˆëŸ¬ì˜¤ê¸°\n","file_path=\"/content/BTS01.csv\"\n","df = pd.read_csv(file_path, encoding=\"utf-8-sig\")  #CSVíŒŒì¼ì„ pandas\n","sentences = df[\"ê°€ì‚¬\"].astype(str).tolist()  # ê°€ì‚¬ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n","\n","#BoWë³€í™˜\n","vectorizer_bow = CountVectorizer()  # BoW ë³€í™˜ê¸° ìƒì„±\n","X_bow = vectorizer_bow.fit_transform(sentences)  # ê°€ì‚¬ ë°ì´í„°ë¥¼ BoWë¡œ ë³€í™˜\n","vocab = vectorizer_bow.get_feature_names_out()  # ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸(ì–´íœ˜ ì‚¬ì „) ìƒì„±\n","df_bow = pd.DataFrame(X_bow.toarray(), columns=vocab)\n","print(df_bow)  # BoW ë°ì´í„° ì¶œë ¥\n","\n","#ê°€ì¥ ë§ì´ ë“±ì¥í•œ ë‹¨ì–´10ê°œ ì°¾ê¸°\n","top_10_words = df_bow.sum().sort_values(ascending=False).head(10)\n","df_top_10_words = pd.DataFrame(top_10_words, columns=[\"ë¹ˆë„ìˆ˜\"]).reset_index()\n","df_top_10_words.rename(columns={\"index\": \"ë‹¨ì–´\"}, inplace=True)\n","\n","\n","# ìƒìœ„ 10ê°œ ë‹¨ì–´ ë°” ì°¨íŠ¸ ì‹œê°í™”\n","plt.figure(figsize=(10, 6))\n","plt.bar(df_top_10_words[\"ë‹¨ì–´\"], df_top_10_words[\"ë¹ˆë„ìˆ˜\"], color=\"royalblue\")\n","plt.xlabel(\"ë‹¨ì–´\")\n","plt.ylabel(\"ë¹ˆë„ìˆ˜\")\n","plt.title(\"BTS ê°€ì‚¬ì—ì„œ ê°€ì¥ ë§ì´ ë“±ì¥í•œ ë‹¨ì–´ TOP 10\")\n","plt.show()"],"metadata":{"id":"5qNf_yG5xqYi"},"execution_count":null,"outputs":[]}]}