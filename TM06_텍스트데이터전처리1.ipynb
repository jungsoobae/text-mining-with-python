{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":["nsdCG9kcFi-d"],"authorship_tag":"ABX9TyNOdkGLJswGDRjVf0r8HCfW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#6.2. 텍스트 정제의 기본 기술"],"metadata":{"id":"sGGJuMeKFYUA"}},{"cell_type":"markdown","source":["##(1) NLTK (Natural Language Toolkit) - 전통적인 NLP 라이브러리"],"metadata":{"id":"XG6JTEmQFf0y"}},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"id":"Ay59EtTT66Vf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab')"],"metadata":{"id":"kbLS2CzzGGX5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##(2) spaCy: 빠르고 효율적인 NLP 라이브러리."],"metadata":{"id":"nsdCG9kcFi-d"}},{"cell_type":"code","source":["pip install spacy"],"metadata":{"id":"p1T8TW6UFi8l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##예제1)\tPunkt 모델 다운로드 및 단어 토큰화- word_tokenize(text)"],"metadata":{"id":"y1Kzpo2QCZeB"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize"],"metadata":{"id":"XH2rAUrTGIy7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('punkt_tab')"],"metadata":{"id":"btc6AWDzGKaY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GNx7gnbM6Iuv"},"outputs":[],"source":["# Punkt 모델 다운로드 (최초 1회 실행 필요)\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","\n","text = \"I love Python programming.\"\n","tokens = word_tokenize(text)\n","print(tokens)"]},{"cell_type":"markdown","source":["##예제2)\t문장 단위 토큰화- sent_tokenize()"],"metadata":{"id":"Lnflvzo5I1m6"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import sent_tokenize\n","# 필요한 리소스 다운로드\n","nltk.download('punkt')\n","texts = \"Python is amazing. I love coding! Do you like programming?\"\n","# 문장 단위로 토큰화\n","sentences = sent_tokenize(texts)\n","print(sentences)\n"],"metadata":{"id":"EpPcx9iU-f1K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##예제3)\tNLTK에서 제공하는 영어 불용어 목록"],"metadata":{"id":"_I0ibmJAJNSr"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","\n","# 필요한 데이터 다운로드\n","nltk.download('stopwords')\n","\n","# 영어 불용어 목록 확인\n","english_stopwords = stopwords.words('english')\n","print(english_stopwords)"],"metadata":{"id":"zAK11_lg_I9O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##예제4)\t불용어 제거"],"metadata":{"id":"wJkRfVlMJa-4"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import nltk\n","\n","# 필요한 리소스 다운로드\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# 샘플 텍스트 데이터\n","text = \"I love programming in Python. It's amazing to work on data science projects.\"\n","\n","# 1. 텍스트 토큰화\n","tokens = word_tokenize(text)\n","\n","# 2. 불용어 목록 가져오기\n","stop_words = set(stopwords.words('english'))\n","\n","# 3. 불용어 제거 (대소문자 구분 유지)\n","filtered_tokens = [word for word in tokens if word not in stop_words] #간결한 리스트 내포 문법 형식\n","\n","# for word in tokens:\n","#     if word not in stop_words:  # 조건: word가 불용어 목록에 없으면\n","#         filtered_tokens.append(word)  # 리스트에 추가\n","\n","# 결과 출력\n","print(\"Original Tokens:\", tokens)\n","print(\"Filtered Tokens:\", filtered_tokens)"],"metadata":{"id":"_SzEmwlPELc8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##예제5)\t문자열 함수 lower()로 소문자 변환"],"metadata":{"id":"Zed392f6KHT6"}},{"cell_type":"code","source":["text = \"Python is Amazing\"\n","lower_text = text.lower()\n","print(lower_text)"],"metadata":{"id":"yxlNxt2aMne5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##예제6)\t정규 표현식을 활용한 특수 문자 제거"],"metadata":{"id":"sJA6kzZbKOhb"}},{"cell_type":"code","source":["import re\n","# 샘플 텍스트\n","text = \"I love Python! :) #coding 😊\"\n","# 특수문자 및 불필요한 기호 제거\n","cleaned_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n","print(\"Original Text:\", text)\n","print(\"Cleaned Text:\", cleaned_text)\n"],"metadata":{"id":"jqQLFomDKSeq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##예제7)\t어간추출- PorterStemmer"],"metadata":{"id":"_XKDAxQpLvV0"}},{"cell_type":"code","source":["# NLTK에서 제공하는 PorterStemmer 클래스를 가져옵니다.\n","from nltk.stem import PorterStemmer\n","# PorterStemmer 객체를 생성합니다.\n","stemmer = PorterStemmer()\n","# 어간 추출을 수행할 단어를 정의합니다.\n","word = 'running'\n","# 'running' 단어의 어간을 추출합니다.\n","stemmed_word = stemmer.stem(word)\n","# 추출된 어간을 출력합니다.\n","print(stemmed_word)\n"],"metadata":{"id":"aYnKQNc_OhZk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##예제8)\t표제어 추출(Lemmatization)- WordNetLemmatizer"],"metadata":{"id":"fi0lAyCdL1AD"}},{"cell_type":"code","source":["# NLTK 라이브러리 임포트\n","import nltk\n","\n","# WordNetLemmatizer 클래스 가져오기\n","from nltk.stem import WordNetLemmatizer\n","\n","# WordNet 리소스 다운로드 (표제어 추출에 필요)\n","nltk.download('wordnet')\n","\n","# WordNetLemmatizer 객체 생성\n","lemmatizer = WordNetLemmatizer()\n","\n","# 표제어 추출을 수행할 단어 정의\n","word = 'am'\n","\n","# 'am' 단어를 동사(pos='v')로 간주하여 표제어를 추출\n","lemma = lemmatizer.lemmatize(word, pos='v')\n","\n","# 추출된 표제어 출력\n","print(lemma)  # 출력 결과: 'be'"],"metadata":{"id":"iLetpF59SOUJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","#6.3 정규 표현식으로 텍스트 다듬기"],"metadata":{"id":"q-rY_NB1MN1o"}},{"cell_type":"markdown","source":["##예제9)\t이메일 주소 추출"],"metadata":{"id":"smtX3LQ1MR93"}},{"cell_type":"code","source":["import re\n","# 검색할 텍스트\n","text = \"My email is example@gmail.com\"\n","# 이메일 주소를 찾는 정규 표현식 패턴\n","pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n","# 첫 번째 매칭된 이메일 주소 찾기\n","match = re.search(pattern, text)\n","# 결과 출력\n","if match:\n","    print(match.group())  # example@gmail.com\n"],"metadata":{"id":"GVSj7KVJYw_e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##예제10)\t모든 매칭된 패턴 찾기"],"metadata":{"id":"rLDoLlh4MZE6"}},{"cell_type":"code","source":["import re\n","# 검색할 텍스트\n","text = \"Contact us at info@test.com and support@company.org\"\n","emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n","print(emails)\n"],"metadata":{"id":"nq6q0RJ7Bkbp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"m_I0osJxMilI"}},{"cell_type":"code","source":["text = \"Contact us at info@test.com and support@company.org\"\n","emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n","print(emails)  # ['info@test.com', 'support@company.org']"],"metadata":{"id":"7zx2XIl_Dg0x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##예제11)\t문자열 치환"],"metadata":{"id":"YtNFn1fnMnZc"}},{"cell_type":"code","source":["import re\n","# 원본 문자열\n","text = \"Price is $100 and $200\"\n","# 정규식 패턴: '$' 기호 뒤에 오는 숫자 찾기\n","pattern = r\"\\$\\d+\"\n","# 패턴을 \"XXX\"로 치환\n","result = re.sub(pattern, \"XXX\", text)\n","print(result)\n"],"metadata":{"id":"GHTKpnG9Eneg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##예제12)\t특정 패턴을 기준으로 문자열 분리"],"metadata":{"id":"z4tqQx5VMsFW"}},{"cell_type":"code","source":["import re\n","# 원본 문자열\n","text = \"apple,banana;grape|orange\"\n","# 패턴을 기준으로 분리 (콤마, 세미콜론, 파이프)\n","result = re.split(r\"[,;|]\", text)\n","# 결과 출력\n","print(result)\n"],"metadata":{"id":"lyWkzs8XFMbn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","#6.4. 실전 퀘스트 : 나만의 전처리 파이프라인 만들기"],"metadata":{"id":"q64hbjx7M5Tr"}},{"cell_type":"markdown","source":["##예제13)\tNLP 전처리 파이프라인 구축하기"],"metadata":{"id":"KIlSQ9coM7wP"}},{"cell_type":"code","source":["# 필요한 라이브러리 설치 및 임포트\n","import pandas as pd\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","import nltk\n","\n","# 필요한 데이터 다운로드\n","nltk.download('stopwords')\n","\n","# 1. 간단한 텍스트 데이터셋 생성\n","data = {\n","    \"text\": [\n","        \"I love programming with Python! 😊👍\",\n","        \"This is an example of text preprocessing. #NLP\",\n","        \"123 numbers and @special #characters should be removed!\",\n","        \"NLTK is great for natural language processing.\"\n","    ]\n","}\n","\n","# 데이터셋을 데이터프레임으로 변환\n","df = pd.DataFrame(data)\n","\n","# 2. 불필요한 기호 및 숫자 제거\n","def clean_text(text):\n","    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # 알파벳과 공백만 남김\n","    text = re.sub(r\"\\s+\", \" \", text)  # 연속된 공백 제거\n","    text = text.strip()  # 앞뒤 공백 제거\n","    return text\n","\n","df['cleaned_text'] = df['text'].apply(clean_text)\n","print(df['cleaned_text'] )\n","# 3. 불용어 제거\n","stop_words = set(stopwords.words('english'))\n","def remove_stopwords(text):\n","    words = text.split()\n","    filtered_words = [word for word in words if word.lower() not in stop_words]\n","    return \" \".join(filtered_words)\n","\n","df['no_stopwords'] = df['cleaned_text'].apply(remove_stopwords)\n","print(df['no_stopwords'] )\n","# 4. 어간 추출(Stemming)\n","stemmer = PorterStemmer()\n","def stem_words(text):\n","    words = text.split()\n","    stemmed_words = [stemmer.stem(word) for word in words]\n","    return \" \".join(stemmed_words)\n","\n","df['stemmed_text'] = df['no_stopwords'].apply(stem_words)\n","print(df['stemmed_text'] )\n","# 5. 결과 출력\n","print(\"전처리 결과:\")\n","print(df)\n","# 결과를 CSV 파일로 저장 (옵션)\n","df.to_csv(\"preprocessed_text.csv\", index=False)"],"metadata":{"id":"sVmxy-_Eq0a5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","#6.5. 도전 과제"],"metadata":{"id":"5B9KAMRGNIL2"}},{"cell_type":"markdown","source":["##예제14)\t\"Let It Be 가사 속 'Let it be'는 몇 번 나올까?\""],"metadata":{"id":"tqcD-MFPN1j8"}},{"cell_type":"code","source":["# \"Let It Be\" 가사\n","file_path = \"/content/let_it_be.txt\"  # 가사 파일 경로\n","with open(file_path, 'r') as file:\n","    lyrics = file.read()"],"metadata":{"id":"8Jpi6RRyrHed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","# \"Let It Be\" 가사\n","file_path = \"/content/let_it_be.txt\"  # 가사 파일 경로\n","with open(file_path, 'r') as file:\n","    lyrics = file.read()\n","\n","# 1. 텍스트를 소문자로 변환\n","lyrics = lyrics.lower()\n","\n","# 2. 특수문자를 제거하고 공백 정리\n","cleaned_lyrics = re.sub(r\"[^\\w\\s]\", \"\", lyrics)\n","\n","# 3. \"let it be\" 등장 횟수 계산\n","count = len(re.findall(r\"\\blet it be\\b\", cleaned_lyrics))\n","\n","# 4. 결과 출력\n","print(f\"'Let it be'는 총 {count}번 등장합니다.\")"],"metadata":{"id":"TlOQ8tBGjrcO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#1. 필요 라이브러리\n","import re\n","\n","#2 \"Let It Be\" 가사 파일 읽기\n","file_path = \"/content/let_it_be.txt\"  # 가사 파일 경로\n","with open(file_path, 'r') as file:\n","    lyrics = file.read()\n","\n","#3. 데이터 정제 및 전처리\n","lyrics = lyrics.lower() #텍스트를 소문자로 변환\n","\n","#특수문자를 제거하고 공백 정리\n","cleaned_lyrics = re.sub(r\"[^\\w\\s]\", \"\", lyrics)\n","\n","# 4. \"let it be\" 등장 횟수 계산\n","count = len(re.findall(r\"\\blet it be\\b\", cleaned_lyrics))\n","\n","# 5. 결과 출력\n","print(f\"'Let it be'는 총 {count}번 등장합니다.\")\n"],"metadata":{"id":"AKHnCvA_mWCR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"id":"ms7RtiK0n7x6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#📝예제15 \t\"Let It Be 가사 속 자주 등장하는 단어는?"],"metadata":{"id":"PMZ9UA8nL9J7"}},{"cell_type":"code","source":["# 라이브러리 임포트\n","import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from collections import Counter\n","import nltk\n","import matplotlib.pyplot as plt\n","\n","# NLTK 리소스 다운로드 (불용어 리스트 및 토큰화 데이터)\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# \"Let It Be\" 가사 파일 로드\n","file_path = \"/content/let_it_be.txt\"\n","with open(file_path, 'r', encoding='utf-8') as file:\n","    lyrics = file.read()\n","\n","# 1. 소문자로 변환 (대소문자 차이 제거)\n","lyrics = lyrics.lower()\n","\n","# 2. 단어 단위로 분리 (토큰화)\n","tokens = word_tokenize(lyrics)\n","\n","# 3. 특수 문자 제거 (알파벳과 단어만 남김)\n","tokens = [re.sub(r\"[^\\w']\", \"\", token) for token in tokens if token.strip()]\n","\n","# 4. 불용어(Stopwords) 제거 및 커스텀 불용어 추가\n","stop_words = set(stopwords.words('english'))\n","custom_stop_words = {\"let\", \"be\"}  # 가사에서 자주 반복되는 단어 제거\n","stop_words.update(custom_stop_words)\n","filtered_tokens = [token for token in tokens if token and token not in stop_words]\n","\n","# 5. 단어 빈도 계산\n","word_counts = Counter(filtered_tokens)\n","\n","# 6. 상위 10개 단어 추출\n","top_words = word_counts.most_common(10)\n","words, counts = zip(*top_words)\n","print(\"원본 토큰 상위 50개:\\n\", tokens[:50])  # 원래 가사에서 토큰화된 단어 확인\n","print(\"불용어 제거 후 단어 상위 50개:\\n\", filtered_tokens[:50])  # 필터링 후 남은 단어 확인\n","print(\"총 남은 단어 개수:\", len(filtered_tokens))  # 불용어 제거 후 남은 단어 개수 확인\n","print(\"상위 10개 단어:\\n\", top_words)\n","\n","\n","\n","\n","\n","# 7. 단어 빈도 그래프 시각화\n","\n","plt.bar(words, counts, color='skyblue')\n","plt.title(\"Top 10 Most Frequent Words in 'Let It Be'\", fontsize=16)\n","plt.xlabel(\"Words\")\n","plt.ylabel(\"Frequency\")\n","plt.xticks(rotation=45)\n","plt.show()\n"],"metadata":{"id":"SWTox15tn2Qu"},"execution_count":null,"outputs":[]}]}