{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":["nsdCG9kcFi-d"],"authorship_tag":"ABX9TyNOdkGLJswGDRjVf0r8HCfW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#6.2. í…ìŠ¤íŠ¸ ì •ì œì˜ ê¸°ë³¸ ê¸°ìˆ "],"metadata":{"id":"sGGJuMeKFYUA"}},{"cell_type":"markdown","source":["##(1) NLTK (Natural Language Toolkit) - ì „í†µì ì¸ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬"],"metadata":{"id":"XG6JTEmQFf0y"}},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"id":"Ay59EtTT66Vf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab')"],"metadata":{"id":"kbLS2CzzGGX5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##(2) spaCy: ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬."],"metadata":{"id":"nsdCG9kcFi-d"}},{"cell_type":"code","source":["pip install spacy"],"metadata":{"id":"p1T8TW6UFi8l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ì˜ˆì œ1)\tPunkt ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë‹¨ì–´ í† í°í™”- word_tokenize(text)"],"metadata":{"id":"y1Kzpo2QCZeB"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize"],"metadata":{"id":"XH2rAUrTGIy7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('punkt_tab')"],"metadata":{"id":"btc6AWDzGKaY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GNx7gnbM6Iuv"},"outputs":[],"source":["# Punkt ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ ì‹¤í–‰ í•„ìš”)\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","\n","text = \"I love Python programming.\"\n","tokens = word_tokenize(text)\n","print(tokens)"]},{"cell_type":"markdown","source":["##ì˜ˆì œ2)\të¬¸ì¥ ë‹¨ìœ„ í† í°í™”- sent_tokenize()"],"metadata":{"id":"Lnflvzo5I1m6"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import sent_tokenize\n","# í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ\n","nltk.download('punkt')\n","texts = \"Python is amazing. I love coding! Do you like programming?\"\n","# ë¬¸ì¥ ë‹¨ìœ„ë¡œ í† í°í™”\n","sentences = sent_tokenize(texts)\n","print(sentences)\n"],"metadata":{"id":"EpPcx9iU-f1K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ì˜ˆì œ3)\tNLTKì—ì„œ ì œê³µí•˜ëŠ” ì˜ì–´ ë¶ˆìš©ì–´ ëª©ë¡"],"metadata":{"id":"_I0ibmJAJNSr"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","\n","# í•„ìš”í•œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n","nltk.download('stopwords')\n","\n","# ì˜ì–´ ë¶ˆìš©ì–´ ëª©ë¡ í™•ì¸\n","english_stopwords = stopwords.words('english')\n","print(english_stopwords)"],"metadata":{"id":"zAK11_lg_I9O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ì˜ˆì œ4)\të¶ˆìš©ì–´ ì œê±°"],"metadata":{"id":"wJkRfVlMJa-4"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import nltk\n","\n","# í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë°ì´í„°\n","text = \"I love programming in Python. It's amazing to work on data science projects.\"\n","\n","# 1. í…ìŠ¤íŠ¸ í† í°í™”\n","tokens = word_tokenize(text)\n","\n","# 2. ë¶ˆìš©ì–´ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n","stop_words = set(stopwords.words('english'))\n","\n","# 3. ë¶ˆìš©ì–´ ì œê±° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ìœ ì§€)\n","filtered_tokens = [word for word in tokens if word not in stop_words] #ê°„ê²°í•œ ë¦¬ìŠ¤íŠ¸ ë‚´í¬ ë¬¸ë²• í˜•ì‹\n","\n","# for word in tokens:\n","#     if word not in stop_words:  # ì¡°ê±´: wordê°€ ë¶ˆìš©ì–´ ëª©ë¡ì— ì—†ìœ¼ë©´\n","#         filtered_tokens.append(word)  # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n","\n","# ê²°ê³¼ ì¶œë ¥\n","print(\"Original Tokens:\", tokens)\n","print(\"Filtered Tokens:\", filtered_tokens)"],"metadata":{"id":"_SzEmwlPELc8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ì˜ˆì œ5)\të¬¸ìì—´ í•¨ìˆ˜ lower()ë¡œ ì†Œë¬¸ì ë³€í™˜"],"metadata":{"id":"Zed392f6KHT6"}},{"cell_type":"code","source":["text = \"Python is Amazing\"\n","lower_text = text.lower()\n","print(lower_text)"],"metadata":{"id":"yxlNxt2aMne5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ì˜ˆì œ6)\tì •ê·œ í‘œí˜„ì‹ì„ í™œìš©í•œ íŠ¹ìˆ˜ ë¬¸ì ì œê±°"],"metadata":{"id":"sJA6kzZbKOhb"}},{"cell_type":"code","source":["import re\n","# ìƒ˜í”Œ í…ìŠ¤íŠ¸\n","text = \"I love Python! :) #coding ğŸ˜Š\"\n","# íŠ¹ìˆ˜ë¬¸ì ë° ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ì œê±°\n","cleaned_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n","print(\"Original Text:\", text)\n","print(\"Cleaned Text:\", cleaned_text)\n"],"metadata":{"id":"jqQLFomDKSeq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ì˜ˆì œ7)\tì–´ê°„ì¶”ì¶œ- PorterStemmer"],"metadata":{"id":"_XKDAxQpLvV0"}},{"cell_type":"code","source":["# NLTKì—ì„œ ì œê³µí•˜ëŠ” PorterStemmer í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n","from nltk.stem import PorterStemmer\n","# PorterStemmer ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n","stemmer = PorterStemmer()\n","# ì–´ê°„ ì¶”ì¶œì„ ìˆ˜í–‰í•  ë‹¨ì–´ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n","word = 'running'\n","# 'running' ë‹¨ì–´ì˜ ì–´ê°„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n","stemmed_word = stemmer.stem(word)\n","# ì¶”ì¶œëœ ì–´ê°„ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n","print(stemmed_word)\n"],"metadata":{"id":"aYnKQNc_OhZk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ì˜ˆì œ8)\tí‘œì œì–´ ì¶”ì¶œ(Lemmatization)- WordNetLemmatizer"],"metadata":{"id":"fi0lAyCdL1AD"}},{"cell_type":"code","source":["# NLTK ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n","import nltk\n","\n","# WordNetLemmatizer í´ë˜ìŠ¤ ê°€ì ¸ì˜¤ê¸°\n","from nltk.stem import WordNetLemmatizer\n","\n","# WordNet ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (í‘œì œì–´ ì¶”ì¶œì— í•„ìš”)\n","nltk.download('wordnet')\n","\n","# WordNetLemmatizer ê°ì²´ ìƒì„±\n","lemmatizer = WordNetLemmatizer()\n","\n","# í‘œì œì–´ ì¶”ì¶œì„ ìˆ˜í–‰í•  ë‹¨ì–´ ì •ì˜\n","word = 'am'\n","\n","# 'am' ë‹¨ì–´ë¥¼ ë™ì‚¬(pos='v')ë¡œ ê°„ì£¼í•˜ì—¬ í‘œì œì–´ë¥¼ ì¶”ì¶œ\n","lemma = lemmatizer.lemmatize(word, pos='v')\n","\n","# ì¶”ì¶œëœ í‘œì œì–´ ì¶œë ¥\n","print(lemma)  # ì¶œë ¥ ê²°ê³¼: 'be'"],"metadata":{"id":"iLetpF59SOUJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","#6.3 ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë‹¤ë“¬ê¸°"],"metadata":{"id":"q-rY_NB1MN1o"}},{"cell_type":"markdown","source":["##ì˜ˆì œ9)\tì´ë©”ì¼ ì£¼ì†Œ ì¶”ì¶œ"],"metadata":{"id":"smtX3LQ1MR93"}},{"cell_type":"code","source":["import re\n","# ê²€ìƒ‰í•  í…ìŠ¤íŠ¸\n","text = \"My email is example@gmail.com\"\n","# ì´ë©”ì¼ ì£¼ì†Œë¥¼ ì°¾ëŠ” ì •ê·œ í‘œí˜„ì‹ íŒ¨í„´\n","pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n","# ì²« ë²ˆì§¸ ë§¤ì¹­ëœ ì´ë©”ì¼ ì£¼ì†Œ ì°¾ê¸°\n","match = re.search(pattern, text)\n","# ê²°ê³¼ ì¶œë ¥\n","if match:\n","    print(match.group())  # example@gmail.com\n"],"metadata":{"id":"GVSj7KVJYw_e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ì˜ˆì œ10)\tëª¨ë“  ë§¤ì¹­ëœ íŒ¨í„´ ì°¾ê¸°"],"metadata":{"id":"rLDoLlh4MZE6"}},{"cell_type":"code","source":["import re\n","# ê²€ìƒ‰í•  í…ìŠ¤íŠ¸\n","text = \"Contact us at info@test.com and support@company.org\"\n","emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n","print(emails)\n"],"metadata":{"id":"nq6q0RJ7Bkbp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"m_I0osJxMilI"}},{"cell_type":"code","source":["text = \"Contact us at info@test.com and support@company.org\"\n","emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n","print(emails)  # ['info@test.com', 'support@company.org']"],"metadata":{"id":"7zx2XIl_Dg0x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ì˜ˆì œ11)\të¬¸ìì—´ ì¹˜í™˜"],"metadata":{"id":"YtNFn1fnMnZc"}},{"cell_type":"code","source":["import re\n","# ì›ë³¸ ë¬¸ìì—´\n","text = \"Price is $100 and $200\"\n","# ì •ê·œì‹ íŒ¨í„´: '$' ê¸°í˜¸ ë’¤ì— ì˜¤ëŠ” ìˆ«ì ì°¾ê¸°\n","pattern = r\"\\$\\d+\"\n","# íŒ¨í„´ì„ \"XXX\"ë¡œ ì¹˜í™˜\n","result = re.sub(pattern, \"XXX\", text)\n","print(result)\n"],"metadata":{"id":"GHTKpnG9Eneg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ì˜ˆì œ12)\tíŠ¹ì • íŒ¨í„´ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ìì—´ ë¶„ë¦¬"],"metadata":{"id":"z4tqQx5VMsFW"}},{"cell_type":"code","source":["import re\n","# ì›ë³¸ ë¬¸ìì—´\n","text = \"apple,banana;grape|orange\"\n","# íŒ¨í„´ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬ (ì½¤ë§ˆ, ì„¸ë¯¸ì½œë¡ , íŒŒì´í”„)\n","result = re.split(r\"[,;|]\", text)\n","# ê²°ê³¼ ì¶œë ¥\n","print(result)\n"],"metadata":{"id":"lyWkzs8XFMbn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","#6.4. ì‹¤ì „ í€˜ìŠ¤íŠ¸ : ë‚˜ë§Œì˜ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë§Œë“¤ê¸°"],"metadata":{"id":"q64hbjx7M5Tr"}},{"cell_type":"markdown","source":["##ì˜ˆì œ13)\tNLP ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•í•˜ê¸°"],"metadata":{"id":"KIlSQ9coM7wP"}},{"cell_type":"code","source":["# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸\n","import pandas as pd\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","import nltk\n","\n","# í•„ìš”í•œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n","nltk.download('stopwords')\n","\n","# 1. ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±\n","data = {\n","    \"text\": [\n","        \"I love programming with Python! ğŸ˜ŠğŸ‘\",\n","        \"This is an example of text preprocessing. #NLP\",\n","        \"123 numbers and @special #characters should be removed!\",\n","        \"NLTK is great for natural language processing.\"\n","    ]\n","}\n","\n","# ë°ì´í„°ì…‹ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n","df = pd.DataFrame(data)\n","\n","# 2. ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ë° ìˆ«ì ì œê±°\n","def clean_text(text):\n","    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # ì•ŒíŒŒë²³ê³¼ ê³µë°±ë§Œ ë‚¨ê¹€\n","    text = re.sub(r\"\\s+\", \" \", text)  # ì—°ì†ëœ ê³µë°± ì œê±°\n","    text = text.strip()  # ì•ë’¤ ê³µë°± ì œê±°\n","    return text\n","\n","df['cleaned_text'] = df['text'].apply(clean_text)\n","print(df['cleaned_text'] )\n","# 3. ë¶ˆìš©ì–´ ì œê±°\n","stop_words = set(stopwords.words('english'))\n","def remove_stopwords(text):\n","    words = text.split()\n","    filtered_words = [word for word in words if word.lower() not in stop_words]\n","    return \" \".join(filtered_words)\n","\n","df['no_stopwords'] = df['cleaned_text'].apply(remove_stopwords)\n","print(df['no_stopwords'] )\n","# 4. ì–´ê°„ ì¶”ì¶œ(Stemming)\n","stemmer = PorterStemmer()\n","def stem_words(text):\n","    words = text.split()\n","    stemmed_words = [stemmer.stem(word) for word in words]\n","    return \" \".join(stemmed_words)\n","\n","df['stemmed_text'] = df['no_stopwords'].apply(stem_words)\n","print(df['stemmed_text'] )\n","# 5. ê²°ê³¼ ì¶œë ¥\n","print(\"ì „ì²˜ë¦¬ ê²°ê³¼:\")\n","print(df)\n","# ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥ (ì˜µì…˜)\n","df.to_csv(\"preprocessed_text.csv\", index=False)"],"metadata":{"id":"sVmxy-_Eq0a5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","#6.5. ë„ì „ ê³¼ì œ"],"metadata":{"id":"5B9KAMRGNIL2"}},{"cell_type":"markdown","source":["##ì˜ˆì œ14)\t\"Let It Be ê°€ì‚¬ ì† 'Let it be'ëŠ” ëª‡ ë²ˆ ë‚˜ì˜¬ê¹Œ?\""],"metadata":{"id":"tqcD-MFPN1j8"}},{"cell_type":"code","source":["# \"Let It Be\" ê°€ì‚¬\n","file_path = \"/content/let_it_be.txt\"  # ê°€ì‚¬ íŒŒì¼ ê²½ë¡œ\n","with open(file_path, 'r') as file:\n","    lyrics = file.read()"],"metadata":{"id":"8Jpi6RRyrHed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","# \"Let It Be\" ê°€ì‚¬\n","file_path = \"/content/let_it_be.txt\"  # ê°€ì‚¬ íŒŒì¼ ê²½ë¡œ\n","with open(file_path, 'r') as file:\n","    lyrics = file.read()\n","\n","# 1. í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜\n","lyrics = lyrics.lower()\n","\n","# 2. íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•˜ê³  ê³µë°± ì •ë¦¬\n","cleaned_lyrics = re.sub(r\"[^\\w\\s]\", \"\", lyrics)\n","\n","# 3. \"let it be\" ë“±ì¥ íšŸìˆ˜ ê³„ì‚°\n","count = len(re.findall(r\"\\blet it be\\b\", cleaned_lyrics))\n","\n","# 4. ê²°ê³¼ ì¶œë ¥\n","print(f\"'Let it be'ëŠ” ì´ {count}ë²ˆ ë“±ì¥í•©ë‹ˆë‹¤.\")"],"metadata":{"id":"TlOQ8tBGjrcO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#1. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬\n","import re\n","\n","#2 \"Let It Be\" ê°€ì‚¬ íŒŒì¼ ì½ê¸°\n","file_path = \"/content/let_it_be.txt\"  # ê°€ì‚¬ íŒŒì¼ ê²½ë¡œ\n","with open(file_path, 'r') as file:\n","    lyrics = file.read()\n","\n","#3. ë°ì´í„° ì •ì œ ë° ì „ì²˜ë¦¬\n","lyrics = lyrics.lower() #í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜\n","\n","#íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•˜ê³  ê³µë°± ì •ë¦¬\n","cleaned_lyrics = re.sub(r\"[^\\w\\s]\", \"\", lyrics)\n","\n","# 4. \"let it be\" ë“±ì¥ íšŸìˆ˜ ê³„ì‚°\n","count = len(re.findall(r\"\\blet it be\\b\", cleaned_lyrics))\n","\n","# 5. ê²°ê³¼ ì¶œë ¥\n","print(f\"'Let it be'ëŠ” ì´ {count}ë²ˆ ë“±ì¥í•©ë‹ˆë‹¤.\")\n"],"metadata":{"id":"AKHnCvA_mWCR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"id":"ms7RtiK0n7x6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#ğŸ“ì˜ˆì œ15 \t\"Let It Be ê°€ì‚¬ ì† ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ”?"],"metadata":{"id":"PMZ9UA8nL9J7"}},{"cell_type":"code","source":["# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n","import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from collections import Counter\n","import nltk\n","import matplotlib.pyplot as plt\n","\n","# NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ë° í† í°í™” ë°ì´í„°)\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# \"Let It Be\" ê°€ì‚¬ íŒŒì¼ ë¡œë“œ\n","file_path = \"/content/let_it_be.txt\"\n","with open(file_path, 'r', encoding='utf-8') as file:\n","    lyrics = file.read()\n","\n","# 1. ì†Œë¬¸ìë¡œ ë³€í™˜ (ëŒ€ì†Œë¬¸ì ì°¨ì´ ì œê±°)\n","lyrics = lyrics.lower()\n","\n","# 2. ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (í† í°í™”)\n","tokens = word_tokenize(lyrics)\n","\n","# 3. íŠ¹ìˆ˜ ë¬¸ì ì œê±° (ì•ŒíŒŒë²³ê³¼ ë‹¨ì–´ë§Œ ë‚¨ê¹€)\n","tokens = [re.sub(r\"[^\\w']\", \"\", token) for token in tokens if token.strip()]\n","\n","# 4. ë¶ˆìš©ì–´(Stopwords) ì œê±° ë° ì»¤ìŠ¤í…€ ë¶ˆìš©ì–´ ì¶”ê°€\n","stop_words = set(stopwords.words('english'))\n","custom_stop_words = {\"let\", \"be\"}  # ê°€ì‚¬ì—ì„œ ìì£¼ ë°˜ë³µë˜ëŠ” ë‹¨ì–´ ì œê±°\n","stop_words.update(custom_stop_words)\n","filtered_tokens = [token for token in tokens if token and token not in stop_words]\n","\n","# 5. ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°\n","word_counts = Counter(filtered_tokens)\n","\n","# 6. ìƒìœ„ 10ê°œ ë‹¨ì–´ ì¶”ì¶œ\n","top_words = word_counts.most_common(10)\n","words, counts = zip(*top_words)\n","print(\"ì›ë³¸ í† í° ìƒìœ„ 50ê°œ:\\n\", tokens[:50])  # ì›ë˜ ê°€ì‚¬ì—ì„œ í† í°í™”ëœ ë‹¨ì–´ í™•ì¸\n","print(\"ë¶ˆìš©ì–´ ì œê±° í›„ ë‹¨ì–´ ìƒìœ„ 50ê°œ:\\n\", filtered_tokens[:50])  # í•„í„°ë§ í›„ ë‚¨ì€ ë‹¨ì–´ í™•ì¸\n","print(\"ì´ ë‚¨ì€ ë‹¨ì–´ ê°œìˆ˜:\", len(filtered_tokens))  # ë¶ˆìš©ì–´ ì œê±° í›„ ë‚¨ì€ ë‹¨ì–´ ê°œìˆ˜ í™•ì¸\n","print(\"ìƒìœ„ 10ê°œ ë‹¨ì–´:\\n\", top_words)\n","\n","\n","\n","\n","\n","# 7. ë‹¨ì–´ ë¹ˆë„ ê·¸ë˜í”„ ì‹œê°í™”\n","\n","plt.bar(words, counts, color='skyblue')\n","plt.title(\"Top 10 Most Frequent Words in 'Let It Be'\", fontsize=16)\n","plt.xlabel(\"Words\")\n","plt.ylabel(\"Frequency\")\n","plt.xticks(rotation=45)\n","plt.show()\n"],"metadata":{"id":"SWTox15tn2Qu"},"execution_count":null,"outputs":[]}]}