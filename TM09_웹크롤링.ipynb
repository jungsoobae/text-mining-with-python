{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPKEkhXbA+4+2sHPeyY6v2H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#9.2. 웹에서 데이터 가오는 원리"],"metadata":{"id":"QOCT9PvahDM5"}},{"cell_type":"markdown","source":["##예제1)\tHtml 구조 기본 개념"],"metadata":{"id":"JejwAUsoTOGJ"}},{"cell_type":"code","source":["<!DOCTYPE html>\n","<html>\n","<head>\n","    <title>예제 페이지</title>\n","</head>\n","<body>\n","    <h1>안녕하세요, 웹 크롤링!</h1>\n","    <p>이것은 웹 페이지의 예제입니다.</p>\n","    <a href=\"https://example.com\">이동하기</a>\n","    <div class=\"menu\"> 클라스 적용</div>\n","</body>\n","</html>\n"],"metadata":{"id":"-_MEa8E4TQx2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["/* 모든 <p> 태그 */\n","p { color: blue; }\n","/* 클래스가 'menu'인 요소 */\n",".menu { font-size: 16px; }\n","/* id가 'header'인 요소 */\n","#header { background-color: gray; }\n"],"metadata":{"id":"qhqJHK6NhU2A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"HpbP0wp2htBu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NMRergGZQ5xK"},"outputs":[],"source":["!pip install requests"]},{"cell_type":"markdown","source":["##예제2)\thttps://www.google.com 웹페이지 요청"],"metadata":{"id":"cUnapL-wTIYS"}},{"cell_type":"code","source":["import requests\n","# 크롤링할 웹사이트 URL\n","url = \"https://www.dongseo.ac.kr\"\n","# GET 요청으로 웹페이지 가져오기\n","response = requests.get(url)\n","# 응답 코드 출력 (200이면 정상)\n","print(\"응답 코드:\", response.status_code)\n","# 가져온 HTML 일부 출력 (500자)\n","print(response.text[:500])\n"],"metadata":{"id":"YoJSNMg-RJgj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##예제3)\t요청 실패 예제"],"metadata":{"id":"otFEUeTWWbx8"}},{"cell_type":"code","source":["url = \"https://www.google.com/없는페이지\"\n","response = requests.get(url)\n","\n","if response.status_code == 200:\n","    print(\"요청 성공\")\n","    print(response.text[:500])\n","else:\n","    print(f\"요청 실패: {response.status_code}\")\n"],"metadata":{"id":"Oc_kiBwuRiUf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#9.4. HTML을 BeautifulSoup으로 분석하기"],"metadata":{"id":"gETdCj8jidZa"}},{"cell_type":"code","source":["!pip install beautifulsoup4"],"metadata":{"id":"2cgPn2X3Yofl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##예제4)\tHTML 문서 분석"],"metadata":{"id":"JnBA8-yeaHej"}},{"cell_type":"code","source":["from bs4 import BeautifulSoup\n","# 예제 HTML 코드\n","html = \"\"\"\n","<html>\n","    <head><title>웹 크롤링 연습</title></head>\n","    <body>\n","        <h1>안녕하세요!</h1>\n","        <p class=\"info\">이것은 첫 번째 문장입니다.</p>\n","        <p class=\"info\">이것은 두 번째 문장입니다.</p>\n","        <a href=\"https://example.com\">예제 링크</a>\n","    </body>\n","</html>\n","\"\"\"\n","# BeautifulSoup 객체 생성\n","soup = BeautifulSoup(html, 'html.parser')\n","# HTML 요소 추출\n","print(\"페이지 제목:\", soup.title.text)  # <title> 태그 내용 가져오기\n","print(\"헤더 제목:\", soup.h1.text)  # <h1> 태그 내용 가져오기\n","print(\"첫 번째 문단:\", soup.p.text)  # <p> 태그 내용 가져오기\n","print(\"링크 주소:\", soup.a['href'])  # <a> 태그의 href 속성 값 가져오기\n"],"metadata":{"id":"CVZ6NTWxaHG4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##예제5)\t더 많은 데이터 가져오기\n","\n"],"metadata":{"id":"TKJt1S6rbs8w"}},{"cell_type":"code","source":["all_p = soup.find_all(\"p\")  # 모든 <p> 태그 찾기\n","\n","for p in all_p:\n","    print(\"문단:\", p.text)"],"metadata":{"id":"mhdtmtg9bwjP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"wNr-pG_cjJGX"}},{"cell_type":"markdown","source":["#9.5. 실전 예제"],"metadata":{"id":"T8WKHUzxjGkb"}},{"cell_type":"markdown","source":["##예제6)\t회사 홍보 웹사이트의 헤드라인 크롤링"],"metadata":{"id":"Wav9A9Er-W_y"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","\n","# 1. 요청할 URL 설정\n","url = \"https://www.chemstar.kr\"\n","response = requests.get(url)\n","\n","# 2. 웹 페이지 응답 확인\n","if response.status_code == 200:  # HTTP 요청 성공 여부 확인\n","    html = response.text  # 웹 페이지의 HTML 소스 가져오기\n","    soup = BeautifulSoup(html, 'html.parser')  # HTML 소스를 BeautifulSoup을 사용해 파싱\n","\n","    # 3. 특정 태그의 내용 추출\n","    titles = soup.find_all('h1')  # 모든 <h1> 태그를 리스트로 가져오기\n","    print(\"사이트 제목:\")\n","    for idx, title in enumerate(titles):  # 가져온 <h1> 태그를 순회하며 내용 출력\n","        print(f\"{idx + 1}. {title.get_text()}\")  # 태그 내용에서 텍스트만 추출하여 출력\n","else:\n","    # 요청 실패 시 상태 코드 출력\n","    print(f\"웹 페이지를 불러오지 못했습니다. 상태 코드: {response.status_code}\")"],"metadata":{"id":"cTIf9wnHf2Qd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##예제7)\t뉴스 웹사이트에서 기사 제목 크롤링"],"metadata":{"id":"OQBekdgwBu8x"}},{"cell_type":"markdown","source":["###(1) 라이브러리 불러오기"],"metadata":{"id":"2-iz4ZwqoxYD"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import csv\n"],"metadata":{"id":"nnBHlA1ZorAe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###(2) 크롤링할 웹사이트 주소 설정"],"metadata":{"id":"1Hf_8A_3o16r"}},{"cell_type":"code","source":["url = \"https://news.naver.com/section/104\""],"metadata":{"id":"6F2P35Ydo6b-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###(3) 크롤링 차단 방지를 위한 헤더 설정"],"metadata":{"id":"QskqQa_mo9ej"}},{"cell_type":"code","source":["headers = {\n","    \"User-Agent\": \"Mozilla/5.0\"\n","}\n"],"metadata":{"id":"-3-PYHj7pANr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###(4) 웹페이지 요청 (데이터 가져오기)"],"metadata":{"id":"aZIivxEVpGPk"}},{"cell_type":"code","source":["response = requests.get(url, headers=headers)"],"metadata":{"id":"URvDam05pJuN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###(5) 요청이 성공했는지 확인"],"metadata":{"id":"FXF3M_hLpML8"}},{"cell_type":"code","source":["# 4. 요청이 성공했는지 확인\n","if response.status_code == 200:\n","    # 5. HTML을 분석할 수 있는 형태로 변환\n","    soup = BeautifulSoup(response.text, \"html.parser\")\n","    # 6. 뉴스 섹션(id=\"newsct\") 찾기\n","    news_section = soup.find(id=\"newsct\")\n","    if news_section:\n","        # 7. 뉴스 제목 가져오기 (strong 태그 + 클래스 \"sa_text_strong\")\n","        headlines = news_section.find_all(\"strong\", class_=\"sa_text_strong\")\n","        # 8. 뉴스 제목 출력\n","        print(\"네이버 뉴스 (세계 섹션) 헤드라인:\")\n","        news_list = []\n","        for idx, headline in enumerate(headlines, start=1):\n","            title = headline.text.strip()\n","            print(f\"{idx}. {title}\")\n","            news_list.append([idx, title])  # CSV 저장을 위해 리스트에 추가\n","        # 9. CSV 파일로 저장\n","        with open(\"naver_news_headlines_si.csv\", \"w\", newline=\"\", encoding=\"utf-8-sig\") as file:\n","            writer = csv.writer(file)\n","            writer.writerow([\"번호\", \"뉴스 제목\"])  # CSV 헤더 추가\n","            writer.writerows(news_list)  # 뉴스 제목 데이터 저장\n","        print(\"\\n뉴스 헤드라인이 'naver_news_headlines.csv' 파일에 저장되었습니다.\")\n","    else:\n","        print(\"❌ 뉴스 섹션을 찾을 수 없습니다.\")\n","else:\n","    print(f\"❌ 페이지를 불러올 수 없습니다. 상태 코드: {response.status_code}\")\n"],"metadata":{"id":"VkAs-VA8pOyW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import csv\n","# 1. 크롤링할 네이버 뉴스 \"세계\" 섹션 URL\n","url = \"https://news.naver.com/section/104\"\n","# 2. 크롤링 차단 방지를 위한 헤더 설정\n","headers = {\n","    \"User-Agent\": \"Mozilla/5.0\"\n","}\n","# 3. 웹페이지 요청 (데이터 가져오기)\n","response = requests.get(url, headers=headers)\n","# 4. 요청이 성공했는지 확인\n","if response.status_code == 200:\n","    # 5. HTML을 분석할 수 있는 형태로 변환\n","    soup = BeautifulSoup(response.text, \"html.parser\")\n","    # 6. 뉴스 섹션(id=\"newsct\") 찾기\n","    news_section = soup.find(id=\"newsct\")\n","    if news_section:\n","        # 7. 뉴스 제목 가져오기 (strong 태그 + 클래스 \"sa_text_strong\")\n","        headlines = news_section.find_all(\"strong\", class_=\"sa_text_strong\")\n","        # 8. 뉴스 제목 출력\n","        print(\"네이버 뉴스 (세계 섹션) 헤드라인:\")\n","        news_list = []\n","        for idx, headline in enumerate(headlines, start=1):\n","            title = headline.text.strip()\n","            print(f\"{idx}. {title}\")\n","            news_list.append([idx, title])  # CSV 저장을 위해 리스트에 추가\n","        # 9. CSV 파일로 저장\n","        with open(\"naver_news_headlines.csv\", \"w\", newline=\"\", encoding=\"utf-8-sig\") as file:\n","            writer = csv.writer(file)\n","            writer.writerow([\"번호\", \"뉴스 제목\"])  # CSV 헤더 추가\n","            writer.writerows(news_list)  # 뉴스 제목 데이터 저장\n","        print(\"\\n뉴스 헤드라인이 'naver_news_headlines.csv' 파일에 저장되었습니다.\")\n","    else:\n","        print(\"❌ 뉴스 섹션을 찾을 수 없습니다.\")\n","else:\n","    print(f\"❌ 페이지를 불러올 수 없습니다. 상태 코드: {response.status_code}\")\n"],"metadata":{"id":"EHS-Kad5Bw91"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##예제8)\t네이버 뉴스 제목에서 단어 빈도수 분석"],"metadata":{"id":"OfgEtXB0v3uB"}},{"cell_type":"code","source":["!apt-get install -y fonts-nanum\n","!fc-cache -fv\n","!rm -rf ~/.cache/matplotlib\n"],"metadata":{"id":"9hf7q7y7mthj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(news_list)"],"metadata":{"id":"9wVZVzyGmjVu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","\n","#뉴스 제목 리스트에서 텍스트만 추출\n","news_titles = []\n","for item in news_list:\n","    news_titles.append(item[1])  # 뉴스 제목(두 번째 요소)만 저장\n","\n","#모든 뉴스 제목을 하나의 문자열로 합친 후 단어 리스트 생성\n","all_words = \" \".join(news_titles).split()\n","\n","# 3️⃣ 단어 빈도수 계산\n","word_counts = Counter(all_words)\n","\n","# 4️⃣ 가장 많이 등장한 단어 TOP 5 출력\n","print(\"가장 많이 등장한 단어 TOP 5:\")\n","for word, count in word_counts.most_common(5):\n","    print(f\"{word}: {count}회\")\n"],"metadata":{"id":"GTi-syEXv4bl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install wordcloud"],"metadata":{"id":"cjDLbdRW3FQR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!apt-get update\n","!apt-get install -y fonts-nanum\n","!fc-cache -fv\n","!rm ~/.cache/matplotlib -rf"],"metadata":{"id":"KW8uMTly3aNB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##예제8) 최종코드"],"metadata":{"id":"mJ6GuduxrbmG"}},{"cell_type":"code","source":["#1.필요한 라이브러리 불러오기\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","import csv\n","\n","#2.한글폰트 설정\n","font_path = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n","\n","#3.CSV 파일에서 뉴스 제목 불러오기\n","news_titles = []\n","with open(\"naver_news_headlines.csv\", \"r\", encoding=\"utf-8-sig\") as file:\n","    reader = csv.reader(file)\n","    next(reader)  # 첫 번째 줄 (헤더) 건너뛰기\n","    for row in reader:\n","        news_titles.append(row[1])  # 뉴스 제목(두 번째 요소)만 리스트에 추가\n","\n","#4.모든 뉴스 제목을 하나의 문자열로 합친 후 단어 리스트 생성\n","all_words = \" \".join(news_titles).split()\n","\n","#5.단어 빈도수 계산\n","word_counts = Counter(all_words)\n","\n","#6.가장 많이 등장한 단어 TOP 5 출력\n","print(\"가장 많이 등장한 단어 TOP 5:\")\n","for word, count in word_counts.most_common(5):\n","    print(f\"{word}: {count}회\")\n","\n","#7.워드 클라우드 생성\n","wordcloud = WordCloud(font_path=font_path, background_color=\"white\", width=800, height=400).generate_from_frequencies(word_counts)\n","\n","#8.워드 클라우드 출력\n","plt.figure(figsize=(10, 5))\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis(\"off\")  # 축 제거\n","plt.show()"],"metadata":{"id":"spbRoZE63BSE"},"execution_count":null,"outputs":[]}]}